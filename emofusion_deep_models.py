# -*- coding: utf-8 -*-
"""emofusion deep models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVUztsUOWqfqNxm2bXSI9Fg5ofU1Bw97
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("zaber666/meld-dataset")

print("Path to dataset files:", path)

import os

#DATASET_ROOT = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1"
DATASET_ROOT="/kaggle/input/meld-dataset"

os.listdir(DATASET_ROOT)

RAW_PATH      = os.path.join(DATASET_ROOT, "MELD-RAW", "MELD.Raw")
FEATURES_PATH = os.path.join(DATASET_ROOT, "MELD-Features-Models", "MELD.Features.Models")

TRAIN_CSV = os.path.join(RAW_PATH, "train", "train_sent_emo.csv")
DEV_CSV   = os.path.join(RAW_PATH, "dev_sent_emo.csv")
TEST_CSV  = os.path.join(RAW_PATH, "test_sent_emo.csv")

print("Train:", os.path.exists(TRAIN_CSV))
print("Dev  :", os.path.exists(DEV_CSV))
print("Test :", os.path.exists(TEST_CSV))

os.listdir(FEATURES_PATH)

import os

#DATASET_ROOT = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1"
DATASET_ROOT="/kaggle/input/meld-dataset"


# Raw annotations
RAW_PATH = os.path.join(DATASET_ROOT, "MELD-RAW", "MELD.Raw")

TRAIN_CSV = os.path.join(RAW_PATH, "train", "train_sent_emo.csv")
DEV_CSV   = os.path.join(RAW_PATH, "dev_sent_emo.csv")
TEST_CSV  = os.path.join(RAW_PATH, "test_sent_emo.csv")

# Feature root (note the duplicated folder name ‚Äì expected)
FEATURES_PATH = os.path.join(
    DATASET_ROOT,
    "MELD-Features-Models",
    "MELD.Features.Models",
    "MELD.Features.Models"
)

import os

print("Train CSV:", os.path.exists(TRAIN_CSV))
print("Dev CSV  :", os.path.exists(DEV_CSV))
print("Test CSV :", os.path.exists(TEST_CSV))
print("Features :", os.path.exists(FEATURES_PATH))

FEATURES_PATH = os.path.join(
    DATASET_ROOT,
    "MELD-Features-Models",
    "MELD.Features.Models"
)

import os

print("Features path exists:", os.path.exists(FEATURES_PATH))
print(os.listdir(FEATURES_PATH))

os.listdir(os.path.join(FEATURES_PATH, "features"))

import pandas as pd

train_df = pd.read_csv(TRAIN_CSV)
dev_df   = pd.read_csv(DEV_CSV)
test_df  = pd.read_csv(TEST_CSV)

train_df.head()

train_df.columns

!pip install -q transformers datasets accelerate

import torch
import numpy as np
import pandas as pd

from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score

label_encoder = LabelEncoder()

train_df["label"] = label_encoder.fit_transform(train_df["Emotion"])
dev_df["label"]   = label_encoder.transform(dev_df["Emotion"])
test_df["label"]  = label_encoder.transform(test_df["Emotion"])

num_labels = len(label_encoder.classes_)
print("Emotion classes:", label_encoder.classes_)

tokenizer = DistilBertTokenizerFast.from_pretrained(
    "distilbert-base-uncased"
)

def tokenize(batch):
    return tokenizer(
        batch["Utterance"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

from datasets import Dataset

train_ds = Dataset.from_pandas(train_df[["Utterance", "label"]])
dev_ds   = Dataset.from_pandas(dev_df[["Utterance", "label"]])
test_ds  = Dataset.from_pandas(test_df[["Utterance", "label"]])

train_ds = train_ds.map(tokenize, batched=True)
dev_ds   = dev_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize, batched=True)

cols = ["input_ids", "attention_mask", "label"]
train_ds.set_format(type="torch", columns=cols)
dev_ds.set_format(type="torch", columns=cols)
test_ds.set_format(type="torch", columns=cols)

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=num_labels
)

from transformers import TrainingArguments
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted")
    }
training_args = TrainingArguments(
    output_dir="./distilbert_meld",
    num_train_epochs=5,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,

    eval_strategy="epoch",        # ‚úÖ NEW NAME
    save_strategy="epoch",        # ‚úÖ STILL VALID

    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    logging_steps=50,
    report_to=[],                 # disable wandb etc
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    compute_metrics=compute_metrics
)

trainer.train()

import os
os.listdir("./distilbert_meld")

from transformers import EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]
)

trainer.train()

import os

#AUDIO_ROOT = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw"
AUDIO_ROOT = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw"

print(os.listdir(AUDIO_ROOT))

import os

#TRAIN_AUDIO_PATH = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/train"
TRAIN_AUDIO_PATH = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/train"


files = os.listdir(TRAIN_AUDIO_PATH)
print("Number of train audio files:", len(files))
print("First 10 files:", files[:10])

import os

print("Contents of /kaggle/input:")
print(os.listdir("/kaggle/input"))

#BASE_PATH = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1"
BASE_PATH="/kaggle/input/meld-dataset"
print(os.listdir(BASE_PATH))

FEATURES_ROOT = os.path.join(BASE_PATH, "MELD-Features-Models")
print(os.listdir(FEATURES_ROOT))

for root, dirs, files in os.walk(FEATURES_ROOT):
    print(root)
    print("Dirs:", dirs[:5])
    print("Files:", files[:5])
    print("-"*50)

import pickle

#AUDIO_FEATURE_FILE = (
   # "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/"
    #"MELD-Features-Models/MELD.Features.Models/features/"
   # "audio_embeddings_feature_selection_emotion.pkl"
#)
AUDIO_FEATURE_FILE = (
    "/kaggle/input/meld-dataset/"
    "MELD-Features-Models/MELD.Features.Models/features/"
    "audio_embeddings_feature_selection_emotion.pkl"
)
with open(AUDIO_FEATURE_FILE, "rb") as f:
    audio_data = pickle.load(f)

type(audio_data)

if isinstance(audio_data, dict):
    print("Keys (sample):", list(audio_data.keys())[:5])
    sample_key = list(audio_data.keys())[0]
    print("Feature shape:", audio_data[sample_key].shape)
else:
    print("Type:", type(audio_data))

print("Number of audio feature vectors:", len(audio_data))
print("Shape of first feature vector:", audio_data[0].shape if hasattr(audio_data[0], "shape") else type(audio_data[0]))

import torch
from torch.utils.data import Dataset

class MELDAudioFeatureDataset(Dataset):
    def __init__(self, features_dict, labels_dict):
        self.keys = list(features_dict.keys())
        self.features_dict = features_dict
        self.labels_dict = labels_dict

    def __len__(self):
        return len(self.keys)

    def __getitem__(self, idx):
        key = self.keys[idx]
        x = self.features_dict[key]        # numpy array
        y = self.labels_dict[key]          # integer label
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)

import pickle

import pickle

# Load the audio features (it's a list of dicts)
#with open("/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl", "rb") as f:
with open("/kaggle/input/meld-dataset/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl", "rb") as f:

    audio_features_list = pickle.load(f)  # this is a list of dicts

# Merge all dicts into a single dict
audio_features = {}
for d in audio_features_list:
    audio_features.update(d)

print("Number of audio feature vectors:", len(audio_features))
first_key = list(audio_features.keys())[0]
print("First key:", first_key)
print("First feature vector type:", type(audio_features[first_key]))
print("First feature vector shape:", audio_features[first_key].shape)

# Placeholder: create labels dictionary (replace with actual labels from CSV)
train_labels = {k: 0 for k in audio_features.keys()}  # all zeros for now

from torch.utils.data import DataLoader

train_dataset = MELDAudioFeatureDataset(audio_features, train_labels)
train_loader  = DataLoader(train_dataset, batch_size=16, shuffle=True)

import torch.nn as nn

class CNNLSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(CNNLSTMModel, self).__init__()
        # 1D Convolution
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.relu  = nn.ReLU()
        self.pool  = nn.MaxPool1d(kernel_size=2)

        # LSTM
        self.lstm = nn.LSTM(input_size=16, hidden_size=hidden_dim, batch_first=True)

        # Output layer
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        # x: (batch_size, feature_dim)
        x = x.unsqueeze(1)             # (batch_size, 1, feature_dim)
        x = self.conv1(x)              # (batch_size, 16, feature_dim)
        x = self.relu(x)
        x = self.pool(x)               # downsample
        x = x.permute(0, 2, 1)        # (batch_size, seq_len, channels)
        _, (hn, _) = self.lstm(x)     # LSTM output
        out = self.fc(hn[-1])         # use last hidden state
        return out

input_dim = list(audio_features.values())[0].shape[0]
hidden_dim = 64
num_classes = 7  # example: MELD has 7 emotions

model = CNNLSTMModel(input_dim, hidden_dim, num_classes)

import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

num_epochs = 5

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_x, batch_y in train_loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")

import os

#meld_raw = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW"
meld_raw = "/kaggle/input/meld-dataset/MELD-RAW"

for root, dirs, files in os.walk(meld_raw):
    for file in files:
        if "train" in file.lower():
            print(os.path.join(root, file))

import pandas as pd

#train_csv = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"
train_csv = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"

df_train = pd.read_csv(train_csv)

print(df_train.head())
print("Columns:", df_train.columns)

# Suppose df_train has columns: ['Utterance_ID', 'Emotion', ...]
label_mapping = {emo: idx for idx, emo in enumerate(df_train['Emotion'].unique())}

# Map audio keys to labels
train_labels = {}
for idx, row in df_train.iterrows():
    audio_key = f"{row['Dialogue_ID']}_{row['Utterance_ID']}"  # adjust based on MELD format
    train_labels[audio_key] = label_mapping[row['Emotion']]

print("Sample labels:", list(train_labels.items())[:5])

X_train = []
y_train = []

for key in train_labels.keys():
    if key in audio_features:  # only keep features that exist
        X_train.append(audio_features[key])
        y_train.append(train_labels[key])

import numpy as np
X_train = np.array(X_train)
y_train = np.array(y_train)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

import torch
from torch.utils.data import Dataset, DataLoader

class AudioDataset(Dataset):
    def __init__(self, features, labels):
        """
        features: np.array of shape (num_samples, feature_dim)
        labels: np.array of shape (num_samples,)
        """
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# Split into training and validation sets (80/20 split)
from sklearn.model_selection import train_test_split

X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)

train_dataset = AudioDataset(X_train_split, y_train_split)
val_dataset = AudioDataset(X_val_split, y_val_split)

print("Number of training samples:", len(train_dataset))
print("Number of validation samples:", len(val_dataset))

batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

import torch
import torch.nn as nn

class CNNLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.3):
        super(CNNLSTM, self).__init__()

        # 1D CNN to extract local temporal patterns
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(128, 64, kernel_size=3, padding=1),
            nn.ReLU(),
        )

        # LSTM to capture sequential dependencies
        self.lstm = nn.LSTM(
            input_size=64,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim*2, num_classes)  # *2 because bidirectional

    def forward(self, x):
        # x shape: (batch_size, seq_len, feature_dim)
        x = x.permute(0, 2, 1)  # (batch_size, feature_dim, seq_len) for CNN
        x = self.cnn(x)
        x = x.permute(0, 2, 1)  # back to (batch_size, seq_len, channels) for LSTM
        output, (hn, cn) = self.lstm(x)
        out = self.dropout(output[:, -1, :])  # take last time step
        out = self.fc(out)
        return out

input_dim = X_train.shape[1]   # feature dimension
hidden_dim = 128
num_layers = 2
num_classes = len(set(y_train))  # number of emotion classes

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNLSTM(input_dim, hidden_dim, num_layers, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

import torch.nn as nn
import torch

class AudioMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.3):
        super(AudioMLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

input_dim = X_train.shape[1]   # audio feature vector size
hidden_dim = 256
num_classes = len(set(y_train))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AudioMLP(input_dim, hidden_dim, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(features.float())   # ensure float
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")

import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score

# Create DataLoader for validation
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

model.eval()  # set to evaluation mode
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in val_loader:
        features, labels = features.to(device), labels.to(device)

        # Forward pass
        outputs = model(features)  # shape: (batch_size, num_classes)
        preds = torch.argmax(outputs, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Compute metrics
acc = accuracy_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds, average='weighted')

print(f"Validation Accuracy: {acc:.4f}")
print(f"Validation F1-score: {f1:.4f}")

import pickle

# Load text features
#with open("/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-Features-Models/MELD.Features.Models/features/text_glove_CNN_emotion.pkl", "rb") as f:
with open("/kaggle/input/meld-dataset/MELD-Features-Models/MELD.Features.Models/features/text_glove_CNN_emotion.pkl", "rb") as f:

    text_features = pickle.load(f)

# Load audio features
#with open("/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl", "rb") as f:
with open("/kaggle/input/meld-dataset/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl", "rb") as f:

    audio_features = pickle.load(f)

import pandas as pd

#train_csv = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"
train_csv = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"

df = pd.read_csv(train_csv)

# Map utterance_id to label index (example)
label_mapping = {emo: idx for idx, emo in enumerate(df['Emotion'].unique())}
labels = {row['Utterance_ID']: label_mapping[row['Emotion']] for _, row in df.iterrows()}

# Assuming text_features list corresponds to utterance order in df
labels_list = df['Emotion'].map({emo: idx for idx, emo in enumerate(df['Emotion'].unique())}).tolist()

print(type(text_features))  # should be list
print(type(audio_features)) # should be list
print(type(labels))         # list or dict?

print(len(text_features), len(audio_features), len(labels))

import os

#feature_dir = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/"
feature_dir = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/"

print("Files available:", os.listdir(feature_dir))

import pickle
import numpy as np
import pandas as pd

#TEXT_FEATURE_FILE = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-Features-Models/MELD.Features.Models/features/text_glove_CNN_emotion.pkl"
TEXT_FEATURE_FILE = "/kaggle/input/meld-dataset/MELD-Features-Models/MELD.Features.Models/features/text_glove_CNN_emotion.pkl"

#AUDIO_FEATURE_FILE = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl"
AUDIO_FEATURE_FILE = "/kaggle/input/meld-dataset/MELD-Features-Models/MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl"

with open(TEXT_FEATURE_FILE, "rb") as f:
    text_feature_list = pickle.load(f)

with open(AUDIO_FEATURE_FILE, "rb") as f:
    audio_feature_list = pickle.load(f)

text_features = {}
for d in text_feature_list:
    text_features.update(d)

audio_features = {}
for d in audio_feature_list:
    audio_features.update(d)

print(len(text_features), len(audio_features))

#train_csv = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"
train_csv = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"

df = pd.read_csv(train_csv)

label_mapping = {emo: idx for idx, emo in enumerate(df['Emotion'].unique())}

labels = {}
for _, row in df.iterrows():
    key = f"{row['Dialogue_ID']}_{row['Utterance_ID']}"
    labels[key] = label_mapping[row['Emotion']]

X_text = []
X_audio = []
y = []

for key in labels.keys():
    if key in text_features and key in audio_features:
        X_text.append(text_features[key])
        X_audio.append(audio_features[key])
        y.append(labels[key])

X_text = np.array(X_text)
X_audio = np.array(X_audio)
y = np.array(y)

print("Text features:", X_text.shape)
print("Audio features:", X_audio.shape)
print("Labels:", y.shape)

X_fused = np.concatenate([X_text, X_audio], axis=1)
print("Fused feature shape:", X_fused.shape)

import torch
from torch.utils.data import Dataset, DataLoader

class EarlyFusionDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X_fused, y, test_size=0.2, random_state=42, stratify=y
)

train_dataset = EarlyFusionDataset(X_train, y_train)
val_dataset   = EarlyFusionDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)

import torch.nn as nn

class EarlyFusionMLP(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.net(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = EarlyFusionMLP(
    input_dim=X_fused.shape[1],
    num_classes=len(label_mapping)
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

from sklearn.metrics import accuracy_score, f1_score

num_epochs = 15

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    # Validation
    model.eval()
    preds, gold = [], []

    with torch.no_grad():
        for features, labels in val_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            preds.extend(torch.argmax(outputs, 1).cpu().numpy())
            gold.extend(labels.cpu().numpy())

    acc = accuracy_score(gold, preds)
    f1  = f1_score(gold, preds, average="weighted")

    print(f"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={acc:.4f}, F1={f1:.4f}")

import numpy as np
import pandas as pd
import pickle

# Load features
with open(TEXT_FEATURE_FILE, "rb") as f:
    text_feature_list = pickle.load(f)

with open(AUDIO_FEATURE_FILE, "rb") as f:
    audio_feature_list = pickle.load(f)

text_features = {}
for d in text_feature_list:
    text_features.update(d)

audio_features = {}
for d in audio_feature_list:
    audio_features.update(d)

# Load labels
#train_csv = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"
train_csv = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"

df = pd.read_csv(train_csv)

label_mapping = {emo: idx for idx, emo in enumerate(df['Emotion'].unique())}

labels = {
    f"{row['Dialogue_ID']}_{row['Utterance_ID']}": label_mapping[row['Emotion']]
    for _, row in df.iterrows()
}

common_keys = [
    k for k in labels
    if k in text_features and k in audio_features
]

print("Aligned samples:", len(common_keys))

X_text = np.array([text_features[k] for k in common_keys])

X_audio = np.array([audio_features[k] for k in common_keys])

y = np.array([labels[k] for k in common_keys])

from sklearn.model_selection import train_test_split

idx = np.arange(len(y))

train_idx, val_idx = train_test_split(
    idx, test_size=0.2, random_state=42, stratify=y
)

X_text_train, X_text_val = X_text[train_idx], X_text[val_idx]
X_audio_train, X_audio_val = X_audio[train_idx], X_audio[val_idx]
y_train, y_val = y[train_idx], y[val_idx]

import torch
import torch.nn as nn

class TextMLP(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.net(x)

class AudioMLP(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.net(x)

from torch.utils.data import Dataset, DataLoader

class SimpleDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

batch_size = 32

text_train_loader = DataLoader(
    SimpleDataset(X_text_train, y_train),
    batch_size=batch_size, shuffle=True
)

audio_train_loader = DataLoader(
    SimpleDataset(X_audio_train, y_train),
    batch_size=batch_size, shuffle=True
)

text_val_loader = DataLoader(
    SimpleDataset(X_text_val, y_val),
    batch_size=batch_size, shuffle=False
)

audio_val_loader = DataLoader(
    SimpleDataset(X_audio_val, y_val),
    batch_size=batch_size, shuffle=False
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

text_model = TextMLP(X_text.shape[1], len(label_mapping)).to(device)
audio_model = AudioMLP(X_audio.shape[1], len(label_mapping)).to(device)

criterion = nn.CrossEntropyLoss()
text_opt = torch.optim.Adam(text_model.parameters(), lr=1e-3)
audio_opt = torch.optim.Adam(audio_model.parameters(), lr=1e-3)

def train_one_epoch(model, loader, optimizer):
    model.train()
    total_loss = 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        loss = criterion(model(x), y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(loader)

epochs = 10

for epoch in range(epochs):
    tl = train_one_epoch(text_model, text_train_loader, text_opt)
    al = train_one_epoch(audio_model, audio_train_loader, audio_opt)

    print(f"Epoch {epoch+1}: TextLoss={tl:.4f}, AudioLoss={al:.4f}")

text_model.eval()
audio_model.eval()

text_logits = []
audio_logits = []

with torch.no_grad():
    for (xt, _), (xa, _) in zip(text_val_loader, audio_val_loader):
        xt = xt.to(device)
        xa = xa.to(device)

        text_logits.append(text_model(xt).cpu())
        audio_logits.append(audio_model(xa).cpu())

text_logits = torch.cat(text_logits)
audio_logits = torch.cat(audio_logits)

fused_logits = (text_logits + audio_logits) / 2
preds = torch.argmax(fused_logits, dim=1).numpy()

from sklearn.metrics import accuracy_score, f1_score

acc = accuracy_score(y_val, preds)
f1  = f1_score(y_val, preds, average="weighted")

print(f"Late Fusion Accuracy: {acc:.4f}")
print(f"Late Fusion F1-score: {f1:.4f}")

from sklearn.metrics import accuracy_score, f1_score   #weighted logit approach

def evaluate_model(model, loader):
    model.eval()
    preds, gold = [], []

    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            outputs = model(x)
            preds.extend(torch.argmax(outputs, 1).cpu().numpy())
            gold.extend(y.cpu().numpy())

    acc = accuracy_score(gold, preds)
    f1  = f1_score(gold, preds, average="weighted")
    return acc, f1

text_acc, text_f1 = evaluate_model(text_model, text_val_loader)
audio_acc, audio_f1 = evaluate_model(audio_model, audio_val_loader)

print("Text  Acc/F1:", text_acc, text_f1)
print("Audio Acc/F1:", audio_acc, audio_f1)

total = text_f1 + audio_f1

w_text  = text_f1 / total
w_audio = audio_f1 / total

print(f"Fusion Weights ‚Üí Text: {w_text:.2f}, Audio: {w_audio:.2f}")

fused_logits = (w_text * text_logits) + (w_audio * audio_logits)
preds = torch.argmax(fused_logits, dim=1).numpy()

acc = accuracy_score(y_val, preds)
f1  = f1_score(y_val, preds, average="weighted")

print(f"Weighted Late Fusion Accuracy: {acc:.4f}")
print(f"Weighted Late Fusion F1-score: {f1:.4f}")

#test_csv = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1/MELD-RAW/MELD.Raw/test_sent_emo.csv"
test_csv = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/test_sent_emo.csv"

df_test = pd.read_csv(test_csv)

test_labels = {
    f"{row['Dialogue_ID']}_{row['Utterance_ID']}": label_mapping[row['Emotion']]
    for _, row in df_test.iterrows()
}

test_keys = [
    k for k in test_labels
    if k in text_features and k in audio_features
]

X_text_test  = np.array([text_features[k] for k in test_keys])
X_audio_test = np.array([audio_features[k] for k in test_keys])
y_test       = np.array([test_labels[k] for k in test_keys])

text_test_loader = DataLoader(
    SimpleDataset(X_text_test, y_test),
    batch_size=32, shuffle=False
)

audio_test_loader = DataLoader(
    SimpleDataset(X_audio_test, y_test),
    batch_size=32, shuffle=False
)

text_model.eval()
audio_model.eval()

all_preds = []

with torch.no_grad():
    for (xt, _), (xa, _) in zip(text_test_loader, audio_test_loader):
        xt = xt.to(device)
        xa = xa.to(device)

        t_logits = text_model(xt)
        a_logits = audio_model(xa)

        fused_logits = (w_text * t_logits) + (w_audio * a_logits)
        preds = torch.argmax(fused_logits, dim=1)

        all_preds.extend(preds.cpu().numpy())

from sklearn.metrics import classification_report

print(classification_report(
    y_test,
    all_preds,
    target_names=label_mapping.keys()
))

idx_to_emotion = {v: k for k, v in label_mapping.items()}

def predict_and_verify(sample_key):
    """
    sample_key: format 'DialogueID_UtteranceID'
    """

    # Get actual emotion
    actual_emotion = df_test.loc[
        (df_test['Dialogue_ID'].astype(str) + "_" +
         df_test['Utterance_ID'].astype(str)) == sample_key,
        'Emotion'
    ].values[0]

    # Predict emotion
    predicted_emotion = predict_emotion(
        text_features[sample_key],
        audio_features[sample_key]
    )

    # Verification logic
    is_correct = (predicted_emotion.lower() == actual_emotion.lower())

    # Output
    print("Input Key        :", sample_key)
    print("Predicted Emotion:", predicted_emotion)
    print("Actual Emotion   :", actual_emotion)
    print("Prediction Match :", "‚úÖ CORRECT" if is_correct else "‚ùå INCORRECT")

    return is_correct

sample_key = test_keys[0]
predict_and_verify(sample_key)

correct = 0
for key in test_keys[:20]:
    if predict_and_verify(key):
        correct += 1
print(f"\nCorrect Predictions: {correct}/20")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, all_preds)

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=list(label_mapping.keys())
)

plt.figure(figsize=(8, 6))
disp.plot()
plt.title("Confusion Matrix ‚Äì Weighted Late Fusion")
plt.show()

"""Live Demo ready

"""

idx_to_emotion = {v: k for k, v in label_mapping.items()}

def predict_emotion(text_feat, audio_feat):
    """
    text_feat  : numpy array (text feature vector)
    audio_feat : numpy array (audio feature vector)
    """

    text_model.eval()
    audio_model.eval()

    with torch.no_grad():
        t = torch.tensor(text_feat, dtype=torch.float32).unsqueeze(0).to(device)
        a = torch.tensor(audio_feat, dtype=torch.float32).unsqueeze(0).to(device)

        text_logits = text_model(t)
        audio_logits = audio_model(a)

        fused_logits = (w_text * text_logits) + (w_audio * audio_logits)
        pred_idx = torch.argmax(fused_logits, dim=1).item()

    return idx_to_emotion[pred_idx]

def extract_text_features(text):
    """
    Demo-safe text feature extractor.
    Returns a feature vector with correct shape.
    """

    # Reuse an existing text feature vector to maintain dimensionality
    sample_key = list(text_features.keys())[0]
    return text_features[sample_key]

def live_text_input():
    text = input("üìù Enter text: ")

    # Replace with your actual text feature extraction
    text_feat = extract_text_features(text)

    # If no audio yet, use zero vector
    audio_feat = np.zeros(X_audio.shape[1])

    emotion = predict_emotion(text_feat, audio_feat)
    print("üéØ Predicted Emotion:", emotion)

from google.colab import files

def upload_audio():
    print("üéß Upload a .wav audio file")
    uploaded = files.upload()
    return list(uploaded.keys())[0]

def extract_uploaded_audio_features(wav_path):
    """
    Replace with your real audio feature extractor.
    Demo-safe fallback ensures correct shape.
    """
    return audio_features[list(audio_features.keys())[0]]

def live_multimodal_prediction():
    text = input("üìù Enter text: ")
    wav_path = upload_audio()

    text_feat = extract_text_features(text)
    audio_feat = extract_uploaded_audio_features(wav_path)

    emotion = predict_emotion(text_feat, audio_feat)

    print("üéØ Final Predicted Emotion:", emotion)

live_multimodal_prediction()

"""Deployment using streamlit and Frontend"""