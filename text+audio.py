# -*- coding: utf-8 -*-
"""Text+Audio

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14CThix3WGhEa0yrj-uRBl0uq51urG51P
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("zaber666/meld-dataset")

print("Path to dataset files:", path)

# ===============================
# EmoFusion: Text + Audio (MELD)
# Single-file clean pipeline
# ===============================

import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
from sklearn.utils.class_weight import compute_class_weight


class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.ce = nn.CrossEntropyLoss(reduction="none")

    def forward(self, logits, targets):
        ce_loss = self.ce(logits, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        return focal_loss.mean()


# -------------------------------
# CONFIG
# -------------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 16
EPOCHS_TEXT = 6
EPOCHS_AUDIO = 10
LR_AUDIO = 1e-3
EPOCHS_FUSION = 5
MAX_LEN = 128

#DATA_ROOT = "/root/.cache/kagglehub/datasets/zaber666/meld-dataset/versions/1"
DATA_ROOT = "/kaggle/input/meld-dataset"

TRAIN_CSV = f"{DATA_ROOT}/MELD-RAW/MELD.Raw/train/train_sent_emo.csv"

AUDIO_FEATURE_FILE = (
    f"{DATA_ROOT}/MELD-Features-Models/"
    "MELD.Features.Models/features/"
    "audio_embeddings_feature_selection_emotion.pkl"
)

# -------------------------------
# LOAD & PREPARE DATA
# -------------------------------
df = pd.read_csv(TRAIN_CSV)
df["key"] = df["Dialogue_ID"].astype(str) + "_" + df["Utterance_ID"].astype(str)

# Encode labels
le = LabelEncoder()
df["label"] = le.fit_transform(df["Emotion"])
num_classes = len(le.classes_)

# -------------------------------
# CONTEXT BUILDING (CRITICAL)
# -------------------------------
def build_context(df, window=3):
    contexts = []

    for idx, row in df.iterrows():
        dialogue_id = row["Dialogue_ID"]

        dialogue_df = df[df["Dialogue_ID"] == dialogue_id]
        pos = dialogue_df.index.get_loc(idx)

        start = max(0, pos - window)
        ctx = " ".join(dialogue_df.iloc[start:pos+1]["Utterance"])

        contexts.append(ctx)

    return contexts
df["context"] = build_context(df)

# -------------------------------
# LOAD AUDIO FEATURES
# -------------------------------
with open(AUDIO_FEATURE_FILE, "rb") as f:
    audio_list = pickle.load(f)

audio_features = {}
for d in audio_list:
    audio_features.update(d)

# Align samples
X_text, X_audio, y = [], [], []

for _, row in df.iterrows():
    key = row["key"]
    if key in audio_features:
        X_text.append(row["context"])
        X_audio.append(audio_features[key])
        y.append(row["label"])

X_audio = np.array(X_audio)
y = np.array(y)

# Train/Val split
idx = np.arange(len(y))
train_idx, val_idx = train_test_split(
    idx, test_size=0.2, random_state=42, stratify=y
)

# -------------------------------
# TEXT DATASET
# -------------------------------
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.enc = tokenizer(
            texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt"
        )
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.enc.items()}
        item["labels"] = self.labels[idx]
        return item

# -------------------------------
# AUDIO DATASET
# -------------------------------
class AudioDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# -------------------------------
# CLASS WEIGHTS (FIX NEUTRAL BIAS)
# -------------------------------
weights = compute_class_weight(
    "balanced", classes=np.unique(y), y=y
)
class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)

# -------------------------------
# TEXT MODEL
# -------------------------------
text_model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=num_classes
).to(DEVICE)

text_optimizer = torch.optim.AdamW(text_model.parameters(), lr=2e-5)
text_loss_fn = nn.CrossEntropyLoss(weight=class_weights)

train_text_ds = TextDataset([X_text[i] for i in train_idx], y[train_idx])
val_text_ds   = TextDataset([X_text[i] for i in val_idx], y[val_idx])

train_text_loader = DataLoader(train_text_ds, batch_size=BATCH_SIZE, shuffle=True)
val_text_loader   = DataLoader(val_text_ds, batch_size=BATCH_SIZE)

# ---- Train Text ----
text_model.train()
for epoch in range(EPOCHS_TEXT):
    total = 0
    for batch in train_text_loader:
        batch = {k: v.to(DEVICE) for k, v in batch.items()}
        text_optimizer.zero_grad()
        out = text_model(**batch)
        loss = text_loss_fn(out.logits, batch["labels"])
        loss.backward()
        text_optimizer.step()
        total += loss.item()
    print(f"[Text] Epoch {epoch+1} Loss: {total/len(train_text_loader):.4f}")

# Freeze text model
for p in text_model.parameters():
    p.requires_grad = False

# -------------------------------
# AUDIO MODEL
# -------------------------------
class AudioMLP(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        return self.feature(x)

audio_model = AudioMLP(X_audio.shape[1]).to(DEVICE)


train_audio_ds = AudioDataset(X_audio[train_idx], y[train_idx])
val_audio_ds   = AudioDataset(X_audio[val_idx], y[val_idx])

train_audio_loader = DataLoader(train_audio_ds, batch_size=BATCH_SIZE, shuffle=True)
val_audio_loader   = DataLoader(val_audio_ds, batch_size=BATCH_SIZE)


# ---- Train Audio ----


# Freeze audio model
for p in audio_model.parameters():
    p.requires_grad = False

# -------------------------------
# FUSION MODEL
# -------------------------------

class FusionModel(nn.Module):
    def __init__(self, text_dim=768, audio_dim=256, num_classes=7):
        super().__init__()

        self.net = nn.Sequential(
            nn.Linear(text_dim + audio_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(128, num_classes)
        )

    def forward(self, t_emb, a_emb):
        fused = torch.cat([t_emb, a_emb], dim=1)
        return self.net(fused)




# ---- Train Fusion ----
fusion_model = FusionModel(
    text_dim=768,
    audio_dim=256,
    num_classes=num_classes
).to(DEVICE)

fusion_optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4)
fusion_loss_fn = FocalLoss(gamma=2.0)

fusion_model.train()

for epoch in range(EPOCHS_FUSION):

    preds, gold = [], []

    for i in range(0, len(train_idx), BATCH_SIZE):
        batch_ids = train_idx[i:i+BATCH_SIZE]

        texts = [X_text[j] for j in batch_ids]
        audios = torch.tensor(
            X_audio[batch_ids],
            dtype=torch.float32
        ).to(DEVICE)

        encoded = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LEN
        ).to(DEVICE)

        with torch.no_grad():
            text_outputs = text_model.distilbert(**encoded)
            text_emb = text_outputs.last_hidden_state.mean(dim=1)

            audio_emb = audio_model(audios)

        fusion_optimizer.zero_grad()

        logits = fusion_model(text_emb, audio_emb)
        lbl = torch.tensor(y[batch_ids]).to(DEVICE)

        loss = fusion_loss_fn(logits, lbl)
        loss.backward()
        fusion_optimizer.step()

        preds.extend(logits.argmax(1).cpu().numpy())
        gold.extend(lbl.cpu().numpy())

    f1 = f1_score(gold, preds, average="macro")
    print(f"[Improved Fusion] Epoch {epoch+1} Macro-F1: {f1:.4f}")



# -------------------------------
# FINAL EVALUATION
# -------------------------------
fusion_model.eval()
all_preds, all_gold = [], []

with torch.no_grad():
    for i in range(0, len(val_idx), BATCH_SIZE):
        ids = val_idx[i:i+BATCH_SIZE]

        texts = [X_text[j] for j in ids]
        audios = torch.tensor(
            X_audio[ids], dtype=torch.float32
        ).to(DEVICE)

        encoded = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LEN
        ).to(DEVICE)

        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state[:, 0, :]

        audio_emb = audio_model(audios)

        logits = fusion_model(text_emb, audio_emb)

        preds = logits.argmax(1).cpu().numpy()

        all_preds.extend(preds)
        all_gold.extend(y[ids])

macro_f1 = f1_score(all_gold, all_preds, average="macro")
weighted_f1 = f1_score(all_gold, all_preds, average="weighted")

print("\n=== FINAL REPORT ===")
print("Macro F1    :", round(macro_f1, 4))
print("Weighted F1 :", round(weighted_f1, 4))
print(classification_report(all_gold, all_preds, target_names=le.classes_))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Confusion Matrix
cm = confusion_matrix(all_gold, all_preds)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d",
            xticklabels=le.classes_,
            yticklabels=le.classes_,
            cmap="Blues")

plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Fusion Model")
plt.show()

# -------------------------------
# FINAL EVALUATION (with confidence scores)
# -------------------------------
fusion_model.eval()

all_preds = []
all_gold = []
all_confidences = []

with torch.no_grad():
    for i in range(0, len(val_idx), BATCH_SIZE):
        ids = val_idx[i:i+BATCH_SIZE]

        texts = [X_text[j] for j in ids]
        audios = torch.tensor(
            X_audio[ids], dtype=torch.float32
        ).to(DEVICE)

        encoded = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LEN
        ).to(DEVICE)

        # Text embedding (mean pooling)
        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state.mean(dim=1)

        # Audio embedding
        audio_emb = audio_model(audios)

        # Fusion logits
        logits = fusion_model(text_emb, audio_emb)

        # Convert to probabilities
        probs = torch.softmax(logits, dim=1)

        # Predictions
        preds = torch.argmax(probs, dim=1)

        # Confidence = max softmax probability
        confidences = torch.max(probs, dim=1).values

        all_preds.extend(preds.cpu().numpy())
        all_gold.extend(y[ids])
        all_confidences.extend(confidences.cpu().numpy())

# Metrics
macro_f1 = f1_score(all_gold, all_preds, average="macro")
weighted_f1 = f1_score(all_gold, all_preds, average="weighted")

print("\n=== FINAL REPORT ===")
print("Macro F1    :", round(macro_f1, 4))
print("Weighted F1 :", round(weighted_f1, 4))
print("Avg Confidence :", round(np.mean(all_confidences), 4))

print("\nClassification Report:\n")
print(classification_report(all_gold, all_preds, target_names=le.classes_))

# Optional: show first 10 predictions with confidence
print("\nSample Predictions with Confidence:\n")
for i in range(10):
    print(
        f"Pred: {le.classes_[all_preds[i]]} | "
        f"True: {le.classes_[all_gold[i]]} | "
        f"Confidence: {all_confidences[i]:.4f}"
    )

# -------------------------------
# LOAD TEST DATA (MELD Official Test Set)
# -------------------------------
#TEST_CSV = f"{DATA_ROOT}/MELD-RAW/MELD.Raw/test/test_sent_emo.csv"
TEST_CSV = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/test_sent_emo.csv"


test_df = pd.read_csv(TEST_CSV)
test_df["key"] = test_df["Dialogue_ID"].astype(str) + "_" + test_df["Utterance_ID"].astype(str)

# Encode labels using SAME encoder
test_df["label"] = le.transform(test_df["Emotion"])

# Build dialogue-level context
test_df["context"] = build_context(test_df)

X_test_text = []
X_test_audio = []
y_test = []

for _, row in test_df.iterrows():
    key = row["key"]
    if key in audio_features:
        X_test_text.append(row["context"])
        X_test_audio.append(audio_features[key])
        y_test.append(row["label"])

X_test_audio = np.array(X_test_audio)
y_test = np.array(y_test)

print("Test samples aligned:", len(y_test))

# -------------------------------
# TEST SET EVALUATION
# -------------------------------
fusion_model.eval()

all_preds = []
all_gold = []
all_confidences = []

with torch.no_grad():
    for i in range(0, len(y_test), BATCH_SIZE):
        texts = X_test_text[i:i+BATCH_SIZE]
        audios = torch.tensor(
            X_test_audio[i:i+BATCH_SIZE],
            dtype=torch.float32
        ).to(DEVICE)

        encoded = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=MAX_LEN
        ).to(DEVICE)

        # Text embedding
        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state.mean(dim=1)

        # Audio embedding
        audio_emb = audio_model(audios)

        # Fusion prediction
        logits = fusion_model(text_emb, audio_emb)

        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)
        confidences = torch.max(probs, dim=1).values

        all_preds.extend(preds.cpu().numpy())
        all_gold.extend(y_test[i:i+BATCH_SIZE])
        all_confidences.extend(confidences.cpu().numpy())

# Metrics
macro_f1 = f1_score(all_gold, all_preds, average="macro")
weighted_f1 = f1_score(all_gold, all_preds, average="weighted")

print("\n=== MELD TEST SET RESULTS ===")
print("Macro F1    :", round(macro_f1, 4))
print("Weighted F1 :", round(weighted_f1, 4))
print("Avg Confidence :", round(np.mean(all_confidences), 4))

print("\nClassification Report:\n")
print(classification_report(all_gold, all_preds, target_names=le.classes_))

import os

for root, dirs, files in os.walk("/kaggle/input"):
    for file in files:
        if "test" in file:
            print(os.path.join(root, file))

# ==========================================================
# MELD TEST SAMPLE PREDICTION WITH CONFIDENCE SCORES
# ==========================================================

import torch.nn.functional as F
import random

fusion_model.eval()
text_model.eval()
audio_model.eval()

# ----------------------------------------------------------
# 1️⃣ LOAD TEST CSV
# ----------------------------------------------------------
#TEST_CSV = "/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/test/test_sent_emo.csv"
TEST_CSV="/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw/test_sent_emo.csv"

test_df = pd.read_csv(TEST_CSV)

# Create key
test_df["key"] = (
    test_df["Dialogue_ID"].astype(str) + "_" +
    test_df["Utterance_ID"].astype(str)
)

# ----------------------------------------------------------
# 2️⃣ (OPTIONAL) BUILD CONTEXT FOR TEST SET
# ----------------------------------------------------------
def build_context(df, window=3):
    contexts = []
    for idx, row in df.iterrows():
        dialogue_id = row["Dialogue_ID"]
        dialogue_df = df[df["Dialogue_ID"] == dialogue_id]
        pos = dialogue_df.index.get_loc(idx)

        start = max(0, pos - window)
        ctx = " ".join(dialogue_df.iloc[start:pos+1]["Utterance"])
        contexts.append(ctx)
    return contexts

test_df["context"] = build_context(test_df)

# ----------------------------------------------------------
# 3️⃣ ALIGN TEST TEXT + AUDIO
# ----------------------------------------------------------
X_text_test, X_audio_test, y_test = [], [], []

for _, row in test_df.iterrows():
    key = row["key"]
    if key in audio_features:
        X_text_test.append(row["context"])   # Using contextual text
        X_audio_test.append(audio_features[key])
        y_test.append(le.transform([row["Emotion"]])[0])

X_audio_test = np.array(X_audio_test)
y_test = np.array(y_test)

print("Total aligned test samples:", len(y_test))

# ----------------------------------------------------------
# 4️⃣ RANDOM SAMPLE PREDICTIONS
# ----------------------------------------------------------
num_samples_to_show = 5

print("\n====== SAMPLE TEST PREDICTIONS ======\n")

for _ in range(num_samples_to_show):

    idx = random.randint(0, len(y_test) - 1)

    text = X_text_test[idx]
    true_label = le.classes_[y_test[idx]]

    # Prepare audio tensor
    audio_feat = torch.tensor(
        X_audio_test[idx],
        dtype=torch.float32
    ).unsqueeze(0).to(DEVICE)

    # Tokenize text
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_LEN
    ).to(DEVICE)

    with torch.no_grad():

        # Get CLS embedding
        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state[:, 0, :]

        audio_emb = audio_model(audio_feat)

        logits = fusion_model(text_emb, audio_emb)

        probs = F.softmax(logits, dim=1)

        confidence, pred_idx = torch.max(probs, dim=1)

    pred_label = le.classes_[pred_idx.item()]

    print("Utterance:")
    print(text)
    print("\nTrue Emotion      :", true_label)
    print("Predicted Emotion :", pred_label)
    print("Prediction Confidence:", round(confidence.item(), 4))

    print("\nClass Probabilities:")
    for i, emotion in enumerate(le.classes_):
        print(f"{emotion:10s}: {probs[0][i].item():.4f}")

    print("\n" + "-"*60 + "\n")

# ==========================================================
# REAL-TIME EMOTION PREDICTION (TEXT + AUDIO)
# ==========================================================

import torch.nn.functional as F

fusion_model.eval()
text_model.eval()
audio_model.eval()

def predict_emotion_realtime(text_input, audio_embedding):
    """
    text_input      : string (new utterance)
    audio_embedding : numpy array of shape (audio_dim,)
    """

    # Prepare audio tensor
    audio_tensor = torch.tensor(
        audio_embedding,
        dtype=torch.float32
    ).unsqueeze(0).to(DEVICE)

    # Tokenize text
    encoded = tokenizer(
        text_input,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_LEN
    ).to(DEVICE)

    with torch.no_grad():

        # Extract CLS embedding from DistilBERT
        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state[:, 0, :]

        # Extract audio embedding
        audio_emb = audio_model(audio_tensor)

        # Fusion forward pass
        logits = fusion_model(text_emb, audio_emb)

        probs = F.softmax(logits, dim=1)

        confidence, pred_idx = torch.max(probs, dim=1)

    predicted_emotion = le.classes_[pred_idx.item()]

    # Convert probabilities to dictionary
    prob_dict = {
        emotion: probs[0][i].item()
        for i, emotion in enumerate(le.classes_)
    }

    return predicted_emotion, confidence.item(), prob_dict

# Example: pick random test sample to simulate real-time

import random

idx = random.randint(0, len(X_audio_test) - 1)

sample_text = X_text_test[idx]
sample_audio = X_audio_test[idx]

prediction, confidence, probabilities = predict_emotion_realtime(
    sample_text,
    sample_audio
)

print("Input Text:")
print(sample_text)

print("\nPredicted Emotion:", prediction)
print("Confidence:", round(confidence, 4))

print("\nAll Emotion Probabilities:")
for emotion, prob in probabilities.items():
    print(f"{emotion:10s}: {prob:.4f}")

!pip install sounddevice librosa

!pip install librosa

from google.colab import files

uploaded = files.upload()

input_dim = X_audio.shape[1]

import librosa
import numpy as np

def extract_audio_features(file_path, target_dim):

    # Load audio
    audio, sr = librosa.load(file_path, sr=16000)

    # Extract MFCC (40 coefficients)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)

    # Mean pooling across time
    mfcc_mean = np.mean(mfcc, axis=1)

    # Pad or trim to match training dimension
    if len(mfcc_mean) < target_dim:
        mfcc_mean = np.pad(
            mfcc_mean,
            (0, target_dim - len(mfcc_mean)),
            mode='constant'
        )
    else:
        mfcc_mean = mfcc_mean[:target_dim]

    return mfcc_mean

import torch
import torch.nn.functional as F

def realtime_prediction():

    # 1️⃣ Get live text
    text_input = input("Enter your text: ")

    # 2️⃣ Get uploaded file name
    file_name = input("Enter uploaded .wav filename exactly: ")

    # 3️⃣ Extract audio embedding
    target_dim = X_audio.shape[1]
    audio_embedding = extract_audio_features(file_name, target_dim)

    # Convert to tensor
    audio_tensor = torch.tensor(
        audio_embedding,
        dtype=torch.float32
    ).unsqueeze(0).to(DEVICE)

    # Tokenize text
    encoded = tokenizer(
        text_input,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_LEN
    ).to(DEVICE)

    with torch.no_grad():

        # Text embedding
        text_outputs = text_model.distilbert(**encoded)
        text_emb = text_outputs.last_hidden_state[:, 0, :]

        # Audio embedding
        audio_emb = audio_model(audio_tensor)

        # Fusion prediction
        logits = fusion_model(text_emb, audio_emb)

        probs = F.softmax(logits, dim=1)

        confidence, pred_idx = torch.max(probs, dim=1)

    predicted_emotion = le.inverse_transform(
        [pred_idx.item()]
    )[0]

    confidence_score = confidence.item()

    print("\n========== RESULT ==========")
    print("Predicted Emotion:", predicted_emotion)
    print("Confidence:", round(confidence_score, 4))

    print("\nAll Emotion Probabilities:")
    for i, emotion in enumerate(le.classes_):
        print(f"{emotion:10s}: {probs[0][i].item():.4f}")

realtime_prediction()